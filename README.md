# ğŸƒ **Pumpkin Seeds Classification using Ensemble Machine Learning & MRMR**

This project presents an **Innovative Ensemble Machine Learning Framework** for **Pumpkin Seed Classification** using **Forward Sequential MRMR (Minimum Redundancy Maximum Relevance)** feature engineering.

---

## ğŸ“Œ **Project Overview**

We classified pumpkin seeds of two varieties:
âœ… **ÃœrgÃ¼p Sivrisi**
âœ… **Ã‡erÃ§evelik**

The goal was to **boost accuracy and performance** using **ensemble learning models** combined with **feature selection**.

---

## ğŸ” **Process Workflow**

1ï¸âƒ£ **Data Split**:

* 70% â†’ Training
* 10% â†’ Hold-out Validation
* 20% â†’ Testing

2ï¸âƒ£ **Feature Selection**:

* Applied **MRMR** to select **top relevant features**
* Found that **8 features** are optimal (accuracy drops after 6, so 8 is the sweet spot)

3ï¸âƒ£ **Model Training & Testing**:

* Trained multiple **ensemble models**
* Compared performance using **evaluation metrics**

4ï¸âƒ£ **Best Model**:
â­ **Optimizable Ensemble** achieved the **highest accuracy**

---

## ğŸ›  **Techniques Used**

* **Feature Engineering**: MRMR (Minimum Redundancy Maximum Relevance)

* **Classifiers**:
  âœ… Boosted Trees
  âœ… Bagged Trees
  âœ… RUS Boosted Trees
  âœ… Optimizable Ensemble

* **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score, Specificity

---

## ğŸ“Š **Dataset**

* **Samples**: 2,500 pumpkin seeds
* **Features**: 13 (e.g., Area, Perimeter, Solidity, Aspect Ratio)
* **Classes**:

  * 52% Ã‡erÃ§evelik
  * 48% ÃœrgÃ¼p Sivrisi

---

## âœ… **Key Results**

| ğŸ† **Model**                | **Accuracy (%)** |
| --------------------------- | ---------------- |
| ğŸ”¸ Boosted Trees            | 90.4%            |
| ğŸ”¸ Bagged Trees             | 88.8%            |
| ğŸ”¸ RUS Boosted Trees        | 89.6%            |
| ğŸŒŸ **Optimizable Ensemble** | **90.8%**        |

---

## ğŸ“‚ **Folder Structure**

* ğŸ“ `Dataset/` â†’ Contains CSV or .mat dataset
* ğŸ“ `Results/` â†’ Performance graphs & screenshots
* ğŸ“ `Documentation/` â†’ Abstract, implementation, results, and conclusion

---

## ğŸ–¥ **How to Reproduce**

1. Open **MATLAB**
2. Import the **dataset**
3. Use **Classification Learner App**
4. Apply **MRMR feature selection**
5. Train using **Ensemble models**
6. Evaluate and **export results**

---

## ğŸ” **License**

This work is for **academic review & demonstration**.
No part may be reused, copied, or published without **explicit permission** from the authors.

